# Apache Kafka

---

## ğŸ“Œ Introduction
Apache Kafka is an **open-source distributed event streaming platform** used for building **real-time data pipelines and streaming applications**.  
It is designed to handle **high throughput, fault tolerance, and scalability** while enabling **publish-subscribe** messaging at scale.

---

## âš¡ Key Features
- âš¡ **High Throughput**: Capable of processing millions of events per second.
- ğŸ“ˆ **Scalable**: Easily scales horizontally by adding more brokers.
- ğŸ’¾ **Durable**: Stores streams of records in a distributed, replicated log.
- ğŸ›¡ï¸ **Fault-Tolerant**: Data is replicated across brokers for reliability.
- â±ï¸ **Real-time Processing**: Integrates with stream processing frameworks (e.g., Spark, Flink, Kafka Streams).

---

## ğŸ—ï¸ Kafka Architecture

### ğŸ”‘ Components
1. ğŸ“ **Producer**
    - Applications that publish (write) data to Kafka topics.
    - Example: IoT device sending sensor data.

2. ğŸ‘¨â€ğŸ’» **Consumer**
    - Applications that subscribe (read) data from Kafka topics.
    - Example: Fraud detection system processing transactions.

3. ğŸ“‚ **Topic**
    - Logical channel where records are stored and categorized.
    - Data inside a topic is divided into **partitions**.

4. ğŸ”€ **Partition**
    - Each topic is split into partitions to support parallel processing and scalability.
    - Each partition is an **ordered, immutable sequence of records**.

5. ğŸ–¥ï¸ **Broker**
    - Kafka server that stores data and serves client requests.
    - A Kafka cluster is made up of multiple brokers.

6. ğŸ¦‰ **ZooKeeper (Legacy, being replaced by KRaft)**
    - Manages cluster metadata and leader election.
    - Newer versions (Kafka 2.8+) can run **without ZooKeeper** using **KRaft mode**.

---

## ğŸ”„ Kafka Workflow
1. ğŸ“ **Producers** send data to Kafka topics.
2. ğŸ“‚ **Topics** are divided into **partitions** across brokers.
3. ğŸ‘¨â€ğŸ’» **Consumers** subscribe to topics and read data.
4. ğŸ›¡ï¸ Kafka ensures **fault-tolerance** by replicating partitions across brokers.

---

## ğŸ› ï¸ Use Cases
- ğŸ“Š **Real-Time Analytics** (e.g., financial fraud detection)
- ğŸ“ **Log Aggregation** (centralized event collection)
- ğŸ”„ **Stream Processing** (ETL pipelines, real-time dashboards)
- ğŸ—ï¸ **Event-Driven Architectures** (microservices communication)
- ğŸ”Œ **Data Integration** (moving data between databases, storage, and systems)

---

## ğŸš€ Example: Producer and Consumer in Python

```python
# Producer Example
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers='localhost:9092')
producer.send('test-topic', b'Hello Kafka!')
producer.flush()

# Consumer Example
from kafka import KafkaConsumer

consumer = KafkaConsumer('test-topic',
                         bootstrap_servers='localhost:9092',
                         auto_offset_reset='earliest',
                         group_id='test-group')

for message in consumer:
    print(f"Received: {message.value.decode('utf-8')}")
````

---

## ğŸ“Š Kafka vs Traditional Messaging Systems

| Feature            | Kafka (âš¡)                          | Traditional MQ (ğŸ“¦ RabbitMQ, ActiveMQ) |
| ------------------ | ---------------------------------- | -------------------------------------- |
| Throughput         | âš¡ Very High                        | ğŸ“¦ Moderate                            |
| Storage            | ğŸ’¾ Persistent (disk-based log)     | ğŸ›‘ Usually in-memory                   |
| Scalability        | ğŸ“ˆ Horizontal, partition-based     | ğŸ”’ Limited                             |
| Ordering Guarantee | ğŸ”€ Within partition                | ğŸ“¦ Queue-based                         |
| Use Case           | ğŸ”„ Event streaming, data pipelines | ğŸ“¬ Task scheduling, async processing   |

---
## âš–ï¸ Key Differences

| Feature / Aspect        | ğŸ—‚ï¸ **Autoloader (Databricks)**                          | âš¡ **Apache Kafka**                                |
|--------------------------|----------------------------------------------------------|---------------------------------------------------|
| **Type**                | File-based streaming ingestion                          | Event streaming platform (publish-subscribe model) |
| **Source**              | Cloud storage (S3, ADLS, GCS, etc.)                     | Producers (apps, microservices, IoT devices, etc.) |
| **Data Format**         | CSV, JSON, Parquet, Avro, Delta, etc.                   | Events/records (byte array, JSON, Avro, Protobuf) |
| **Use Case**            | Incremental loading of new/changed files                | Real-time event/message streaming & processing    |
| **Trigger Mode**        | Batch (micro-batch) / Continuous                        | Real-time (low-latency streaming)                 |
| **Schema Evolution**    | âœ… Supports schema inference & evolution                 | âŒ Manual handling (schema registry required)      |
| **Integration**         | Tight integration with **Databricks + Delta Lake**       | Integrates with multiple systems (Spark, Flink, DBs) |
| **Scalability**         | Depends on cloud storage scalability                    | Horizontally scalable via partitions and brokers  |
| **Fault Tolerance**     | âœ… Handled by cloud storage durability                   | âœ… Replication across brokers                     |
| **Best Fit Scenario**   | When data is arriving as **files in cloud storage**      | When data is **event-driven and continuous**      |

